{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Reuse the existing components: InputEmbeddings, PositionalEncoding, LayerNormalization, etc.\n",
    "# Modify only the components necessary for classification.\n",
    "\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size: int, \n",
    "                 src_seq_len: int, \n",
    "                 d_model: int, \n",
    "                 N: int, \n",
    "                 h: int, \n",
    "                 d_ff: int, \n",
    "                 num_classes: int, \n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Input embedding and positional encoding\n",
    "        self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "        self.src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_blocks = []\n",
    "        for _ in range(N):\n",
    "            self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            encoder_block = EncoderBlock(d_model, self_attention_block, feed_forward_block, dropout)\n",
    "            encoder_blocks.append(encoder_block)\n",
    "        self.encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        # Embed and encode the input\n",
    "        src = self.src_embed(src)  # (batch, seq_len, d_model)\n",
    "        src = self.src_pos(src)    # (batch, seq_len, d_model)\n",
    "        encoder_output = self.encoder(src, src_mask)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Aggregate features for classification\n",
    "        # Mean pooling over the sequence length\n",
    "        pooled_output = encoder_output.mean(dim=1)  # (batch, d_model)\n",
    "\n",
    "        # Classification\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Utility to create masks\n",
    "def create_padding_mask(input_tensor, pad_token_idx):\n",
    "    return (input_tensor != pad_token_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "# Hyperparameters and training setup\n",
    "# src_vocab_size = 20000  # Adjust based on your dataset's vocabulary size\n",
    "# src_seq_len = 512       # Maximum sequence length for a paragraph\n",
    "# d_model = 256           # Embedding size\n",
    "# N = 6                   # Number of encoder layers\n",
    "# h = 8                   # Number of attention heads\n",
    "# d_ff = 1024             # Feedforward network size\n",
    "# num_classes = 14        # Number of topic classes\n",
    "# dropout = 0.1\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = TransformerEncoderClassifier(\n",
    "#     src_vocab_size=src_vocab_size,\n",
    "#     src_seq_len=src_seq_len,\n",
    "#     d_model=d_model,\n",
    "#     N=N,\n",
    "#     h=h,\n",
    "#     d_ff=d_ff,\n",
    "#     num_classes=num_classes,\n",
    "#     dropout=dropout\n",
    "# )\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Dummy training loop\n",
    "# def train_model(model, train_loader, num_epochs=10):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             paragraphs, labels = batch\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Create masks (assuming 0 is the PAD token)\n",
    "#             src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khoảng 6h30, thiếu niên ngụ xã Thượng Quận, th...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hai xe đối đầu tạo tiếng động mạnh, ba người đ...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Camera hành trình của ôtô đi trên đoạn đường đ...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camera hành trình ghi lại vụ tai nạn sáng 6/12.</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đoạn đường xảy ra tai nạn không có dải phân cá...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Tiếc thay, đó cũng không phải lần cuối rắc rối...</td>\n",
       "      <td>Ý kiến</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Ngày thắng kiện, tôi lặng lẽ về dọn đồ của mẹ ...</td>\n",
       "      <td>Ý kiến</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>Từ lúc hiểu ra điều đó, tôi đã hoàn toàn \"buôn...</td>\n",
       "      <td>Ý kiến</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>Hiện nay, nhiều người có cái nhìn khá phiến di...</td>\n",
       "      <td>Ý kiến</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>Đầu tiên, cần nhìn nhận rằng suy thoái kinh tế...</td>\n",
       "      <td>Ý kiến</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paragraph    Topic\n",
       "0      Khoảng 6h30, thiếu niên ngụ xã Thượng Quận, th...  Thời sự\n",
       "1      Hai xe đối đầu tạo tiếng động mạnh, ba người đ...  Thời sự\n",
       "2      Camera hành trình của ôtô đi trên đoạn đường đ...  Thời sự\n",
       "3        Camera hành trình ghi lại vụ tai nạn sáng 6/12.  Thời sự\n",
       "4      Đoạn đường xảy ra tai nạn không có dải phân cá...  Thời sự\n",
       "...                                                  ...      ...\n",
       "29995  Tiếc thay, đó cũng không phải lần cuối rắc rối...   Ý kiến\n",
       "29996  Ngày thắng kiện, tôi lặng lẽ về dọn đồ của mẹ ...   Ý kiến\n",
       "29997  Từ lúc hiểu ra điều đó, tôi đã hoàn toàn \"buôn...   Ý kiến\n",
       "29998  Hiện nay, nhiều người có cái nhìn khá phiến di...   Ý kiến\n",
       "29999  Đầu tiên, cần nhìn nhận rằng suy thoái kinh tế...   Ý kiến\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:/AI/S1_Y3/DL/Code/final/data.csv')\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mà_thôi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuy_có</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đâu_như</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quay_bước</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hỏi_lại</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>cái_họ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>hết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>lại</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>thế_thế</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>dần_dần</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1942 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stopwords\n",
       "0       mà_thôi\n",
       "1        tuy_có\n",
       "2       đâu_như\n",
       "3     quay_bước\n",
       "4       hỏi_lại\n",
       "...         ...\n",
       "1937     cái_họ\n",
       "1938        hết\n",
       "1939        lại\n",
       "1940    thế_thế\n",
       "1941    dần_dần\n",
       "\n",
       "[1942 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = pd.read_csv('D:/AI/S1_Y3/DL/Code/final/stopwords.csv')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "import re, os, string\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('<.*?>', '', text).strip()\n",
    "    text = re.sub('(\\s)+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    listpunctuation = string.punctuation.replace('_', '')\n",
    "    for i in listpunctuation:\n",
    "        text = text.replace(i, ' ')\n",
    "    return text.lower()\n",
    "\n",
    "# list stopwords\n",
    "\n",
    "\n",
    "def remove_stopword(text):\n",
    "    pre_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            pre_text.append(word)\n",
    "    text2 = ' '.join(pre_text)\n",
    "\n",
    "    return text2\n",
    "\n",
    "def sentence_segment(text):\n",
    "    sents = re.split(\"([.?!])?[\\n]+|[.?!] \", text)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def word_segment(sent):\n",
    "    sent = tokenize(sent)\n",
    "    return sent\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Full preprocessing pipeline for a single text.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = normalize_text(text)\n",
    "    text = remove_stopword(text)\n",
    "    return text\n",
    "\n",
    "def convert_paragraphs_to_list(dataframe, text_column):\n",
    "    \"\"\"Convert the content of the specified text column to a list of processed strings.\"\"\"\n",
    "    processed_texts = dataframe[text_column].apply(preprocess_text).tolist()\n",
    "    return processed_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    content = clean_text(data.loc[i, 'Paragraph'])\n",
    "    sents = sentence_segment(content)\n",
    "    for sent in sents:\n",
    "        if(sent != None):\n",
    "            sent = word_segment(sent)\n",
    "            sent = remove_stopword(normalize_text(sent))\n",
    "            if(len(sent.split()) > 1):\n",
    "                data.loc[i, 'Paragraph'] = sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(tokenized_texts, min_freq=1, special_tokens=[\"<pad>\", \"<unk>\"]):\n",
    "    \n",
    "    # Flatten the list of tokenized texts and count token frequencies\n",
    "    all_tokens = [token for text in tokenized_texts for token in text.split()]\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Filter tokens by minimum frequency\n",
    "    filtered_tokens = {token: count for token, count in token_counts.items() if count >= min_freq}\n",
    "\n",
    "    # Create the vocabulary with special tokens first\n",
    "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    vocab.update({token: idx + len(vocab) for idx, token in enumerate(filtered_tokens.keys())})\n",
    "\n",
    "    return vocab, filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in text.split()]\n",
    "        tokens = tokens[:self.max_len]\n",
    "        tokens += [self.vocab[\"<pad>\"]] * (self.max_len - len(tokens))\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoded_text = self.encode(text)\n",
    "        return encoded_text, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5957, Accuracy: 0.48\n",
      "Epoch 2/20, Loss: 1.1840, Accuracy: 0.62\n",
      "Epoch 3/20, Loss: 0.9234, Accuracy: 0.73\n",
      "Epoch 4/20, Loss: 0.4716, Accuracy: 0.90\n",
      "Epoch 5/20, Loss: 0.5509, Accuracy: 0.81\n",
      "Epoch 6/20, Loss: 0.2611, Accuracy: 0.92\n",
      "Epoch 7/20, Loss: 0.5173, Accuracy: 0.83\n",
      "Epoch 8/20, Loss: 0.3874, Accuracy: 0.88\n",
      "Epoch 9/20, Loss: 0.2709, Accuracy: 0.90\n",
      "Epoch 10/20, Loss: 0.1915, Accuracy: 0.92\n",
      "Epoch 11/20, Loss: 0.2421, Accuracy: 0.94\n",
      "Epoch 12/20, Loss: 0.1175, Accuracy: 0.94\n",
      "Epoch 13/20, Loss: 0.3078, Accuracy: 0.92\n",
      "Epoch 14/20, Loss: 0.3150, Accuracy: 0.90\n",
      "Epoch 15/20, Loss: 0.0662, Accuracy: 0.98\n",
      "Epoch 16/20, Loss: 0.2467, Accuracy: 0.94\n",
      "Epoch 17/20, Loss: 0.0363, Accuracy: 1.00\n",
      "Epoch 18/20, Loss: 0.2333, Accuracy: 0.92\n",
      "Epoch 19/20, Loss: 0.1589, Accuracy: 0.94\n",
      "Epoch 20/20, Loss: 0.1472, Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy data loader\n",
    "    texts = convert_paragraphs_to_list(data, text_column=\"Paragraph\")\n",
    "\n",
    "    unique_topics = data['Topic'].unique()\n",
    "    topic_to_id = {topic: idx for idx, topic in enumerate(unique_topics)} \n",
    "\n",
    "    data['Topic'] = data['Topic'].map(topic_to_id)\n",
    "    labels = data['Topic'].tolist()\n",
    "    vocab, token_freqs = build_vocab(texts)\n",
    "\n",
    "    # Hyperparameters and training setup\n",
    "    max_len = 15\n",
    "    src_vocab_size = 30000  # Adjust based on your dataset's vocabulary size\n",
    "    src_seq_len = 512       # Maximum sequence length for a paragraph\n",
    "    d_model = 256           # Embedding size\n",
    "    N = 6                   # Number of encoder layers\n",
    "    h = 8                   # Number of attention heads\n",
    "    d_ff = 1024             # Feedforward network size\n",
    "    num_classes = 15        # Number of topic classes\n",
    "    dropout = 0.1           # Dropout rate\n",
    "    batch_size = 64         # Number of samples per batch\n",
    "    epochs = 20             # Number of epochs\n",
    "    learning_rate = 0.001   # Learning rate\n",
    "\n",
    "    dataset = TextDataset(texts, labels, vocab, max_len)\n",
    "\n",
    "    # Split the data into train, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "    train_indices, test_val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(test_val_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create subsets for train, validation, and test\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    # DataLoaders for train, validation, and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = TransformerEncoderClassifier(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        src_seq_len=src_seq_len,\n",
    "        d_model=d_model,\n",
    "        N=N,\n",
    "        h=h,\n",
    "        d_ff=d_ff,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dummy training loop\n",
    "    def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            for batch in train_loader:\n",
    "                paragraphs, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Create masks (assuming 0 is the PAD token)\n",
    "                src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            accuracy = lambda y_pred, y_true: (y_pred.argmax(1) == y_true).float().mean().item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy(outputs, labels):.2f}\")\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    paragraphs, labels = batch\n",
    "                    src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "                    outputs = model(paragraphs, src_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_accuracy += accuracy(outputs, labels)\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_accuracy /= len(val_loader)\n",
    "\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
    "            model.train()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "y_true = [] \n",
    "y_pred = []\n",
    "\n",
    "cm = torch.zeros(num_classes, num_classes, dtype=torch.int32)\n",
    "\n",
    "for texts, labels in test_loader:\n",
    "    outputs = model(texts, create_padding_mask(texts, pad_token_idx=0))\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    y_true.append(labels.item())\n",
    "    y_pred.append(predictions.item())\n",
    "\n",
    "\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "# Step 2: Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm.numpy(), interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels\n",
    "classes = [f\"Class {i}\" for i in range(num_classes)]\n",
    "plt.xticks(ticks=np.arange(num_classes), labels=classes, rotation=45)\n",
    "plt.yticks(ticks=np.arange(num_classes), labels=classes)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Annotate each cell with the count\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, cm[i, j].item(), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Compute metrics for each class (Precision, Recall, F1-score)\n",
    "def compute_metrics(cm):\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i].item()\n",
    "        fp = cm[:, i].sum().item() - tp\n",
    "        fn = cm[i, :].sum().item() - tp\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        metrics[f\"Class {i}\"] = {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "metrics = compute_metrics(cm)\n",
    "\n",
    "# Print metrics for each class\n",
    "for cls, m in metrics.items():\n",
    "    print(f\"{cls}: Precision={m['Precision']:.2f}, Recall={m['Recall']:.2f}, F1-Score={m['F1-Score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
