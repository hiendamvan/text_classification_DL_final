{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:54.953483Z",
     "iopub.status.busy": "2024-12-13T09:14:54.952685Z",
     "iopub.status.idle": "2024-12-13T09:14:57.708619Z",
     "shell.execute_reply": "2024-12-13T09:14:57.707919Z",
     "shell.execute_reply.started": "2024-12-13T09:14:54.953445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.710235Z",
     "iopub.status.busy": "2024-12-13T09:14:57.709874Z",
     "iopub.status.idle": "2024-12-13T09:14:57.765283Z",
     "shell.execute_reply": "2024-12-13T09:14:57.764279Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.710208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.766753Z",
     "iopub.status.busy": "2024-12-13T09:14:57.766486Z",
     "iopub.status.idle": "2024-12-13T09:14:57.777659Z",
     "shell.execute_reply": "2024-12-13T09:14:57.777006Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.766727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.779227Z",
     "iopub.status.busy": "2024-12-13T09:14:57.778952Z",
     "iopub.status.idle": "2024-12-13T09:14:57.786856Z",
     "shell.execute_reply": "2024-12-13T09:14:57.786185Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.779189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.788975Z",
     "iopub.status.busy": "2024-12-13T09:14:57.788705Z",
     "iopub.status.idle": "2024-12-13T09:14:57.795665Z",
     "shell.execute_reply": "2024-12-13T09:14:57.794937Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.788941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.796767Z",
     "iopub.status.busy": "2024-12-13T09:14:57.796545Z",
     "iopub.status.idle": "2024-12-13T09:14:57.805214Z",
     "shell.execute_reply": "2024-12-13T09:14:57.804371Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.796745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.806727Z",
     "iopub.status.busy": "2024-12-13T09:14:57.806460Z",
     "iopub.status.idle": "2024-12-13T09:14:57.819157Z",
     "shell.execute_reply": "2024-12-13T09:14:57.818351Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.806701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.820392Z",
     "iopub.status.busy": "2024-12-13T09:14:57.820062Z",
     "iopub.status.idle": "2024-12-13T09:14:57.831399Z",
     "shell.execute_reply": "2024-12-13T09:14:57.830701Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.820356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.832950Z",
     "iopub.status.busy": "2024-12-13T09:14:57.832643Z",
     "iopub.status.idle": "2024-12-13T09:14:57.842373Z",
     "shell.execute_reply": "2024-12-13T09:14:57.841606Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.832916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.843806Z",
     "iopub.status.busy": "2024-12-13T09:14:57.843474Z",
     "iopub.status.idle": "2024-12-13T09:14:57.850880Z",
     "shell.execute_reply": "2024-12-13T09:14:57.850194Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.843771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.852265Z",
     "iopub.status.busy": "2024-12-13T09:14:57.851974Z",
     "iopub.status.idle": "2024-12-13T09:14:57.861091Z",
     "shell.execute_reply": "2024-12-13T09:14:57.860463Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.852233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Reuse the existing components: InputEmbeddings, PositionalEncoding, LayerNormalization, etc.\n",
    "# Modify only the components necessary for classification.\n",
    "\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size: int, \n",
    "                 src_seq_len: int, \n",
    "                 d_model: int, \n",
    "                 N: int, \n",
    "                 h: int, \n",
    "                 d_ff: int, \n",
    "                 num_classes: int, \n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Input embedding and positional encoding\n",
    "        self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "        self.src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_blocks = []\n",
    "        for _ in range(N):\n",
    "            self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            encoder_block = EncoderBlock(d_model, self_attention_block, feed_forward_block, dropout)\n",
    "            encoder_blocks.append(encoder_block)\n",
    "        self.encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        # Embed and encode the input\n",
    "        src = self.src_embed(src)  # (batch, seq_len, d_model)\n",
    "        src = self.src_pos(src)    # (batch, seq_len, d_model)\n",
    "        encoder_output = self.encoder(src, src_mask)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Aggregate features for classification\n",
    "        # Mean pooling over the sequence length\n",
    "        pooled_output = encoder_output.mean(dim=1)  # (batch, d_model)\n",
    "\n",
    "        # Classification\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Utility to create masks\n",
    "def create_padding_mask(input_tensor, pad_token_idx):\n",
    "    return (input_tensor != pad_token_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "# Hyperparameters and training setup\n",
    "# src_vocab_size = 20000  # Adjust based on your dataset's vocabulary size\n",
    "# src_seq_len = 512       # Maximum sequence length for a paragraph\n",
    "# d_model = 256           # Embedding size\n",
    "# N = 6                   # Number of encoder layers\n",
    "# h = 8                   # Number of attention heads\n",
    "# d_ff = 1024             # Feedforward network size\n",
    "# num_classes = 14        # Number of topic classes\n",
    "# dropout = 0.1\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = TransformerEncoderClassifier(\n",
    "#     src_vocab_size=src_vocab_size,\n",
    "#     src_seq_len=src_seq_len,\n",
    "#     d_model=d_model,\n",
    "#     N=N,\n",
    "#     h=h,\n",
    "#     d_ff=d_ff,\n",
    "#     num_classes=num_classes,\n",
    "#     dropout=dropout\n",
    "# )\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Dummy training loop\n",
    "# def train_model(model, train_loader, num_epochs=10):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             paragraphs, labels = batch\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Create masks (assuming 0 is the PAD token)\n",
    "#             src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:57.862483Z",
     "iopub.status.busy": "2024-12-13T09:14:57.862137Z",
     "iopub.status.idle": "2024-12-13T09:14:58.219584Z",
     "shell.execute_reply": "2024-12-13T09:14:58.218890Z",
     "shell.execute_reply.started": "2024-12-13T09:14:57.862448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.221011Z",
     "iopub.status.busy": "2024-12-13T09:14:58.220576Z",
     "iopub.status.idle": "2024-12-13T09:14:58.224851Z",
     "shell.execute_reply": "2024-12-13T09:14:58.223929Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.220974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# stop_words = pd.read_csv('D:/AI/S1_Y3/DL/Code/BTL/data/stopwords.csv')\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.229107Z",
     "iopub.status.busy": "2024-12-13T09:14:58.228803Z",
     "iopub.status.idle": "2024-12-13T09:14:58.612932Z",
     "shell.execute_reply": "2024-12-13T09:14:58.612078Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.229070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>khoảng thiếu_niên ngụ xã thượng quận thị_xã ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hai xe đối_đầu tạo tiếng_động mạnh ba người đi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>camera hành_trình của ôtô đi trên đoạn đường đ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>camera hành_trình ghi lại vụ tai_nạn sáng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>đoạn đường xảy ra tai_nạn không có dải_phân_cá...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>tiếc thay đó cũng không phải lần cuối rắc_rối ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ngày thắng kiện tôi lặng_lẽ về dọn đồ của mẹ r...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>từ lúc hiểu ra_điều đó tôi đã hoàn_toàn buông_...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>hiện_nay nhiều người có cái nhìn khá phiến_diệ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>đầu_tiên cần nhìn_nhận rằng suy_thoái kinh_tế ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          Paragraph  Topic\n",
       "0               0  khoảng thiếu_niên ngụ xã thượng quận thị_xã ki...      0\n",
       "1               1  hai xe đối_đầu tạo tiếng_động mạnh ba người đi...      0\n",
       "2               2  camera hành_trình của ôtô đi trên đoạn đường đ...      0\n",
       "3               3          camera hành_trình ghi lại vụ tai_nạn sáng      0\n",
       "4               4  đoạn đường xảy ra tai_nạn không có dải_phân_cá...      0\n",
       "...           ...                                                ...    ...\n",
       "29995       29995  tiếc thay đó cũng không phải lần cuối rắc_rối ...     14\n",
       "29996       29996  ngày thắng kiện tôi lặng_lẽ về dọn đồ của mẹ r...     14\n",
       "29997       29997  từ lúc hiểu ra_điều đó tôi đã hoàn_toàn buông_...     14\n",
       "29998       29998  hiện_nay nhiều người có cái nhìn khá phiến_diệ...     14\n",
       "29999       29999  đầu_tiên cần nhìn_nhận rằng suy_thoái kinh_tế ...     14\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/pre-data/preprocessed_data.csv')\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.614640Z",
     "iopub.status.busy": "2024-12-13T09:14:58.614202Z",
     "iopub.status.idle": "2024-12-13T09:14:58.619634Z",
     "shell.execute_reply": "2024-12-13T09:14:58.618852Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.614593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from pyvi.ViTokenizer import tokenize\n",
    "# import re, os, string\n",
    "# import pandas as pd\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = re.sub('<.*?>', '', text).strip()\n",
    "#     text = re.sub('(\\s)+', r'\\1', text)\n",
    "#     return text\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     listpunctuation = string.punctuation.replace('_', '')\n",
    "#     for i in listpunctuation:\n",
    "#         text = text.replace(i, ' ')\n",
    "#     return text.lower()\n",
    "\n",
    "# # list stopwords\n",
    "\n",
    "\n",
    "# def remove_stopword(text):\n",
    "#     pre_text = []\n",
    "#     words = text.split()\n",
    "#     for word in words:\n",
    "#         if word not in stop_words:\n",
    "#             pre_text.append(word)\n",
    "#     text2 = ' '.join(pre_text)\n",
    "\n",
    "#     return text2\n",
    "\n",
    "# def sentence_segment(text):\n",
    "#     sents = re.split(\"([.?!])?[\\n]+|[.?!] \", text)\n",
    "#     return sents\n",
    "\n",
    "\n",
    "# def word_segment(sent):\n",
    "#     sent = tokenizer.tokenize(sent)\n",
    "#     return sent\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"Full preprocessing pipeline for a single text.\"\"\"\n",
    "#     text = clean_text(text)\n",
    "#     text = normalize_text(text)\n",
    "#     text = remove_stopword(text)\n",
    "#     return text\n",
    "\n",
    "# def convert_paragraphs_to_list(dataframe, text_column):\n",
    "#     \"\"\"Convert the content of the specified text column to a list of processed strings.\"\"\"\n",
    "#     processed_texts = dataframe[text_column].apply(preprocess_text).tolist()\n",
    "#     return processed_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.621099Z",
     "iopub.status.busy": "2024-12-13T09:14:58.620782Z",
     "iopub.status.idle": "2024-12-13T09:14:58.634339Z",
     "shell.execute_reply": "2024-12-13T09:14:58.633552Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.621064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(data)):\n",
    "#     content = clean_text(data.loc[i, 'Paragraph'])\n",
    "#     sents = sentence_segment(content)\n",
    "#     for sent in sents:\n",
    "#         if(sent != None):\n",
    "#             sent = word_segment(sent)\n",
    "#             sent = remove_stopword(normalize_text(sent))\n",
    "#             if(len(sent.split()) > 1):\n",
    "#                 data.loc[i, 'Paragraph'] = sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.635634Z",
     "iopub.status.busy": "2024-12-13T09:14:58.635371Z",
     "iopub.status.idle": "2024-12-13T09:14:58.643847Z",
     "shell.execute_reply": "2024-12-13T09:14:58.642960Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.635598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(tokenized_texts, min_freq=1, special_tokens=[\"<pad>\", \"<unk>\"]):\n",
    "    \n",
    "    # Flatten the list of tokenized texts and count token frequencies\n",
    "    all_tokens = [token for text in tokenized_texts for token in text]\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Filter tokens by minimum frequency\n",
    "    filtered_tokens = {token: count for token, count in token_counts.items() if count >= min_freq}\n",
    "\n",
    "    # Create the vocabulary with special tokens first\n",
    "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    vocab.update({token: idx + len(vocab) for idx, token in enumerate(filtered_tokens.keys())})\n",
    "\n",
    "    return vocab, filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.645038Z",
     "iopub.status.busy": "2024-12-13T09:14:58.644810Z",
     "iopub.status.idle": "2024-12-13T09:14:58.654068Z",
     "shell.execute_reply": "2024-12-13T09:14:58.653281Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.645014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.655491Z",
     "iopub.status.busy": "2024-12-13T09:14:58.655110Z",
     "iopub.status.idle": "2024-12-13T09:14:58.665435Z",
     "shell.execute_reply": "2024-12-13T09:14:58.664631Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.655453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:14:58.666536Z",
     "iopub.status.busy": "2024-12-13T09:14:58.666304Z",
     "iopub.status.idle": "2024-12-13T09:15:10.983765Z",
     "shell.execute_reply": "2024-12-13T09:15:10.982812Z",
     "shell.execute_reply.started": "2024-12-13T09:14:58.666511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2c300a0cfd4f0da09a921b5d0c2d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f73d30a9c9a41539238379752482117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aea87c9a14846448e3ab5d9261507e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5aad25390e421fbf2d0ee8b08af0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "data[\"Texts\"] = data[\"Paragraph\"].apply(\n",
    "lambda x: tokenizer.encode(x, truncation=True, max_length=256, padding=\"max_length\")\n",
    ")\n",
    "texts = data['Texts'].tolist()\n",
    "\n",
    "# unique_topics = data['Topic'].unique()\n",
    "# topic_to_id = {topic: idx for idx, topic in enumerate(unique_topics)} \n",
    "\n",
    "# data['Topic'] = data['Topic'].map(topic_to_id)\n",
    "labels = data['Topic'].tolist()\n",
    "\n",
    "# vocab, token_freqs = build_vocab(tokenized_texts, min_freq=1)\n",
    "\n",
    "# Hyperparameters and training setup\n",
    "max_len = 15\n",
    "src_vocab_size = tokenizer.vocab_size   # Adjust based on your dataset's vocabulary size\n",
    "src_seq_len = 512                       # Maximum sequence length for a paragraph\n",
    "d_model = 256                           # Embedding size\n",
    "N = 6                                   # Number of encoder layers\n",
    "h = 8                                   # Number of attention heads\n",
    "d_ff = 1024                             # Feedforward network size\n",
    "num_classes = 15                        # Number of topic classes\n",
    "dropout = 0.2                           # Dropout rate\n",
    "batch_size = 64                         # Number of samples per batch\n",
    "epochs = 20                             # Number of epochs\n",
    "learning_rate = 0.001                   # Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:15:10.985918Z",
     "iopub.status.busy": "2024-12-13T09:15:10.985396Z",
     "iopub.status.idle": "2024-12-13T09:38:30.424126Z",
     "shell.execute_reply": "2024-12-13T09:38:30.423227Z",
     "shell.execute_reply.started": "2024-12-13T09:15:10.985877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.3642, Accuracy: 0.61\n",
      "Validation Loss: 1.3331, Validation Accuracy: 0.62\n",
      "Epoch 2/20, Loss: 0.7719, Accuracy: 0.75\n",
      "Validation Loss: 1.1398, Validation Accuracy: 0.69\n",
      "Epoch 3/20, Loss: 0.6756, Accuracy: 0.80\n",
      "Validation Loss: 1.0114, Validation Accuracy: 0.74\n",
      "Epoch 4/20, Loss: 0.6962, Accuracy: 0.80\n",
      "Validation Loss: 0.9444, Validation Accuracy: 0.75\n",
      "Epoch 5/20, Loss: 0.7182, Accuracy: 0.81\n",
      "Validation Loss: 1.0385, Validation Accuracy: 0.76\n",
      "Epoch 6/20, Loss: 0.3409, Accuracy: 0.89\n",
      "Validation Loss: 1.0513, Validation Accuracy: 0.76\n",
      "Epoch 7/20, Loss: 0.6294, Accuracy: 0.81\n",
      "Validation Loss: 1.0427, Validation Accuracy: 0.78\n",
      "Epoch 8/20, Loss: 0.3636, Accuracy: 0.86\n",
      "Validation Loss: 0.9899, Validation Accuracy: 0.79\n",
      "Epoch 9/20, Loss: 0.2365, Accuracy: 0.91\n",
      "Validation Loss: 1.0962, Validation Accuracy: 0.78\n",
      "Epoch 10/20, Loss: 0.1766, Accuracy: 0.95\n",
      "Validation Loss: 1.0773, Validation Accuracy: 0.79\n",
      "Epoch 11/20, Loss: 0.1325, Accuracy: 0.95\n",
      "Validation Loss: 1.1658, Validation Accuracy: 0.79\n",
      "Epoch 12/20, Loss: 0.2451, Accuracy: 0.92\n",
      "Validation Loss: 1.2008, Validation Accuracy: 0.79\n",
      "Epoch 13/20, Loss: 0.2734, Accuracy: 0.95\n",
      "Validation Loss: 1.1773, Validation Accuracy: 0.80\n",
      "Epoch 14/20, Loss: 0.4244, Accuracy: 0.86\n",
      "Validation Loss: 1.1988, Validation Accuracy: 0.80\n",
      "Epoch 15/20, Loss: 0.2952, Accuracy: 0.91\n",
      "Validation Loss: 1.2431, Validation Accuracy: 0.80\n",
      "Epoch 16/20, Loss: 0.0990, Accuracy: 0.97\n",
      "Validation Loss: 1.2603, Validation Accuracy: 0.79\n",
      "Epoch 17/20, Loss: 0.1347, Accuracy: 0.95\n",
      "Validation Loss: 1.2756, Validation Accuracy: 0.80\n",
      "Epoch 18/20, Loss: 0.0876, Accuracy: 0.97\n",
      "Validation Loss: 1.3392, Validation Accuracy: 0.80\n",
      "Epoch 19/20, Loss: 0.1279, Accuracy: 0.97\n",
      "Validation Loss: 1.3385, Validation Accuracy: 0.80\n",
      "Epoch 20/20, Loss: 0.0695, Accuracy: 0.98\n",
      "Validation Loss: 1.2386, Validation Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy data loader\n",
    "\n",
    "    dataset = TextDataset(texts, labels)\n",
    "\n",
    "    # Split the data into train, validation, and test sets (80% train, 10% validation, 10% test)\n",
    "    train_indices, test_val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(test_val_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create subsets for train, validation, and test\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    # DataLoaders for train, validation, and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    # texts, labels, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    #     temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    "    # )\n",
    "\n",
    "    # train_dataset = TextDataset(train_texts, train_labels)\n",
    "    # val_dataset = TextDataset(val_texts, val_labels)\n",
    "    # test_dataset = TextDataset(test_texts, test_labels)\n",
    "  \n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        # Instantiate the model\n",
    "    model = TransformerEncoderClassifier(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        src_seq_len=src_seq_len,\n",
    "        d_model=d_model,\n",
    "        N=N,\n",
    "        h=h,\n",
    "        d_ff=d_ff,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Dummy training loop\n",
    "    def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            for para, label in train_loader:\n",
    "                paragraphs, labels = para.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Create masks (assuming 0 is the PAD token)\n",
    "                src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            accuracy = lambda y_pred, y_true: (y_pred.argmax(1) == y_true).float().mean().item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy(outputs, labels):.2f}\")\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for para, label in val_loader:\n",
    "                    paragraphs, labels = para.to(device), label.to(device)\n",
    "                    src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n",
    "                    outputs = model(paragraphs, src_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_accuracy += accuracy(outputs, labels)\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_accuracy /= len(val_loader)\n",
    "\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
    "            model.train()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, num_epochs=epochs)\n",
    "    # Save the model's state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:38:30.425670Z",
     "iopub.status.busy": "2024-12-13T09:38:30.425195Z",
     "iopub.status.idle": "2024-12-13T09:38:30.600233Z",
     "shell.execute_reply": "2024-12-13T09:38:30.599537Z",
     "shell.execute_reply.started": "2024-12-13T09:38:30.425643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6291126,
     "sourceId": 10183846,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6292955,
     "sourceId": 10186388,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
